---
description: >-
  This document outlines the various Output types (Streaming, Triggers and Archives)
  supported by the Edge Delta agent, and how the outputs are configured.
---

# Outputs

### Overview

Outputs are the mechanism that tells the Edge Delta agent which destinations to stream data, alerts, notifications, and automation. Outputs come in two forms, **Streaming** destinations, and **Trigger** destinations.

\*\*\*\*[**Streaming destinations**](outputs.md#streaming-destinations) are typically Centralized Monitoring Platforms \(i.e. Splunk, Sumo Logic, Datadog, Snowflake, New Relic, Elastic, etc.\) that Edge Delta can be configured to send the analytics and insights continuously generated by the service.

\*\*\*\*[**Trigger destinations**](outputs.md#trigger-destinations) are alerting and automation systems \(i.e. PagerDuty, Slack, ServiceNow, OpsGenie, Runbook, etc.\) that Edge Delta can be configured to send alerts and notifications when anomalies are detected or various conditions are met.

\*\*\*\*[**Archive destinations**](outputs.md#archive-destinations) are storage services that Edge Delta can be configured to send raw data logs.

## Streaming Destinations

### Features Description

Features are the data types enabled for the streaming destinations. 
These are the data types to use in streaming definitions.

- **log**: log data type is used to enable log forwarding a stream destination.
- **metric**: metric data type enables to send metrics which are populated from ingested raw data with defined processors.
- **edac**: EDAC (Edge Delta Anomaly Context) data type to enable sending contextual logs when an anomaly happened.
- **cluster**: cluster data type enables to send cluster info of ingested raw data which includes "cluster-pattern, count" pairs and cluster samples of each pattern.
- **topk**: topk data type enables to send top k records whose counts are greater than others. 
- **all**: all enables all features for stream destination but note that only supported features will be working by stream destination.

### Splunk

If enabled, the Splunk integration will stream analytics and insights to a Splunk HEC endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Streaming destination type \(i.e. splunk, sumologic, datadog, etc.\) | Yes |
| endpoint | Full Splunk HEC URI for this integration | Yes |
| token | Splunk HEC Token for this integration | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```go
      - name: splunk-integration
        type: splunk
        endpoint: "<protocol>://<host>:<port>/<endpoint>"
        token: "32-character GUID token"
```

**Finding the appropriate HEC URI to provide for endpoint \(Splunk Enterprise, Self-Service Splunk Cloud, Managed Splunk Cloud\):** [https://docs.splunk.com/Documentation/Splunk/8.0.3/Data/UsetheHTTPEventCollector\#Send\_data\_to\_HTTP\_Event\_Collector\_on\_Splunk\_Enterprise](https://docs.splunk.com/Documentation/Splunk/8.0.3/Data/UsetheHTTPEventCollector#Send_data_to_HTTP_Event_Collector_on_Splunk_Enterprise)

**Generate HEC token for a Splunk endpoint:** [https://docs.splunk.com/Documentation/Splunk/8.0.3/Data/UsetheHTTPEventCollector\#About\_Event\_Collector\_tokens](https://docs.splunk.com/Documentation/Splunk/8.0.3/Data/UsetheHTTPEventCollector#About_Event_Collector_tokens)

**Find the HTTP Port Number used for HEC endpoints \(Global Settings\):** [https://docs.splunk.com/Documentation/Splunk/8.0.3/Data/UsetheHTTPEventCollector\#Enable\_HTTP\_Event\_Collector](https://docs.splunk.com/Documentation/Splunk/8.0.3/Data/UsetheHTTPEventCollector#Enable_HTTP_Event_Collector)

### Sumo Logic

If enabled, the Sumo Logic integration will stream analytics and insights to a Sumo Logic HTTPs Endpoint

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Streaming destination type \(i.e. sumologic, datadog, splunk, etc.\) | Yes |
| endpoint | Full HTTPs URL for this endpoint | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```go
      - name: sumo-logic-integration
        type: sumologic
        endpoint: "https://[SumoEndpoint]/receiver/v1/http/[UniqueHTTPCollectorCode]"
```

**Creating a new Sumo Logic HTTPs Endpoint:** [https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source\#access-a-sources-url](https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source#access-a-sources-url) _\*\*_

**Finding an existing Sumo Logic HTTPs Endpoint URL:** [https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source\#access-a-sources-url](https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source#access-a-sources-url) _\*\*_

### Amazon CloudWatch

If enabled, the CloudWatch integration will stream logs to a given aws region.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Streaming destination type \(i.e. sumologic, datadog, splunk, etc.\) | Yes |
| region | AWS region destionation for logs | Yes |
| log\_group\_name | CloudWatch log group name | Yes |
| log\_stream\_name | CloudWatch log stream name \(either name or prefix is supported not both\) | Yes |
| log\_stream\_prefix | CloudWatch log stream prefix \(either name or prefix is supported not both\) | Yes |
| auto\_create | When necessery iam policies provided if auto\_create is set, log group and log stream will be created if not exists | No |
| allow\_label\_override | monitored container can override the default values of log group name, logs stream name and log stream prefix, by setting ed\_log\_group\_name, ed\_log\_stream\_name, ed\_log\_stream\_prefix labels | No |
| auto\_configure | only supported for ECS environments, and when provided only region configuration can be provided. Automatically create LogGroupName in the format of /ecs/task\_definition\_family and LogsStreamPrefix in the format of ecs/container\_name/task\_id | No |
| type | Streaming destination type \(i.e. sumologic, datadog, splunk, etc.\) | Yes |
| features | Features defines which data types stream to backend, it can be "log" only for Amazon Cloudwatch. | No |

```go
      - name: cloudwatch
        type: cloudwatch
        region: us-west-2
        log_group_name: /ecs/microservice
        log_stream_prefix: ecs
        auto_create: true
        features: log
```

* Assign below permission to taskExecutionRoleArn for putting log events into CloudWatch when auto\_create is not set

  ```go
      {
        "Version": "2012-10-17",
        "Statement": [{
          "Effect": "Allow",
          "Action": [
            "logs:PutLogEvents"
          ],
          "Resource": "*"
        }]
      }
  ```

* Assign below permission to taskExecutionRoleArn if auto\_create is set

  ```go
      {
        "Version": "2012-10-17",
        "Statement": [{
          "Effect": "Allow",
          "Action": [
            "logs:CreateLogStream",
            "logs:CreateLogGroup",
            "logs:DescribeLogStreams",
            "logs:PutLogEvents"
          ],
          "Resource": "*"
        }]
      }
  ```

**CloudWatch log group name requirements:** [https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API\_CreateLogGroup.html](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_CreateLogGroup.html) _\*\*_

**CloudWatch log stream name requirements:** [https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API\_CreateLogStream.html](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_CreateLogStream.html) _\*\*_

### **Datadog**

If enabled, the Datadog integration will stream analytics and insights to your Datadog environment

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Streaming destination type \(i.e. datadog, sumologic, splunk, etc.\) | Yes |
| api\_key | Datadog API Key | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```go
      - name: datadog-integration
        type: datadog
        api_key: "<add datadog api key>"
```

**Create a new Datadog API Key:** [https://docs.datadoghq.com/account\_management/api-app-keys/\#add-a-key](https://docs.datadoghq.com/account_management/api-app-keys/#add-a-key)

### **New Relic**

If enabled, the New Relic integration will stream analytics and insights to your New Relic environment

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Streaming destination type \(i.e. newrelic, sumologic, datadog, etc.\) | Yes |
| api\_key | New Relic Insert API Key | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```go
      - name: new-relic-integration
        type: newrelic
        api_key: "<add new relic insert api key>"
```

**Create a new New Relic Insert API Key:** [https://docs.newrelic.com/docs/apis/get-started/intro-apis/types-new-relic-api-keys\#event-insert-key](https://docs.newrelic.com/docs/apis/get-started/intro-apis/types-new-relic-api-keys#event-insert-key)

### **InfluxDB**

If enabled, the InfluxDB integration will stream analytics and insights to your InfluxDB deployment

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Streaming destination type \(i.e. influxdb, sumologic, datadog, etc.\) | Yes |
| endpoint | InfluxDB endpoint | Yes |
| http\_user | InfluxDB user credentials | Yes |
| http\_password | InfluxDB password for connecting user | Yes |
| db | Specific InfluxDB database to stream data to | Yes |
| features | Features defines which data types stream to backend, it can be "metric", "edac". If you don't provide any value then it is all. | No |

```go
      - name: influxdb-integration
        type: influxdb
        endpoint: "https://influxdb.<your-domain>.com/"
        port: 443
        http_user: admin
        http_password: your_http_password
        db: "specific_influxdb_database"
```

### **Wavefront**

If enabled, the Wavefront integration will stream analytics and insights to your Wavefront environment

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Streaming destination type \(i.e. wavefront, influxdb, sumologic, datadog, etc.\) | Yes |
| endpoint | Wavefront endpoint | Yes |
| token | Wavefront API token | Yes |
| features | Features defines which data types stream to backend, it can be "metric" only for Wavefront. | No |

```go
      - name: wavefront-integration
        type: wavefront
        endpoint: "https://{your wavefront domain}.wavefront.com/report"
        token: "<add wavefront api token>"
```

### **Scalyr**

If enabled, the Scalyr integration will stream analytics and insights to your Scalyr environment

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Streaming destination type \(i.e. scalyr, influxdb, sumologic, datadog, etc.\) | Yes |
| endpoint | Scalyr endpoint | Yes |
| features | Features defines which data types stream to backend, it can be "log" for Scalyr. | No |

```go
      - name: scalyr-integration
        type: scalyr
        endpoint: "https://app.scalyr.com/api/uploadLogs?token={scalyr log access write key}"
```

### **Elastic Search**

If enabled, the Elastic Search integration will stream analytics and insights to your Elastic Search environment. Elastic index template and lifecycle creation guide can be found [here](../appendices/elastic-index.md). It's not mandatory but highly recommended to complete those steps in the guide to prepare your Elastic Search environment to be Edgedelta streaming target.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Streaming destination type \(i.e. elastic, scalyr, influxdb, sumologic, datadog, etc.\) | Yes |
| index | Name of elastic index \(or index template\) where data will be streamed by edgedelta agents. Set this to 'ed-agent-log' if followed the guide above | Yes |
| cloud\_id | Cloud ID of elastic search backend | No |
| address | Adress list of elastic search backend | No |
| token | Elastic Search API Key | No |
| user | Username for elastic search credentials | No |
| password | Elastic Search password for connecting user | No |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

* For the connection url, you can not provide cloud\_id and adress at the same time. And you must provide at least one of them.
* For the authentication, you can not provide token and user/password at the same time. And you must provide at least one of them.

```go
      - name: elastic-integration
        type: elastic
        index: "index name"
        # you can provide cloud or adress list but not both at the same time 
        cloud_id: "<add elasticsearch cloud_id>"
        #address:
         #- <elastic search endpoint address_1>
         #- <elastic search endpoint address_2>
        # you can provide token or user/pass for auth but not both at the same time 
        token: "elastic search api key"
        #user: "elastic search username"
        #password: "elastic search password"
```

### Azure AppInsight

If enabled, the Azure AppInsight integration will stream analytics and insights to an Azure endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Streaming destination type \(azure\) | Yes |
| endpoint | Azure AppInsight endpoint. | Yes |
| api_key | Azure AppInsight API key. | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```go
      - name: azure-integration
        type: azure
        endpoint: https://dc.services.visualstudio.com/v2/track
        api_key: "Azure AppInsight api key" 
        features: "metric"
```

### Kafka

If enabled, the Kafka integration will stream analytics and insights to an Kafka endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Streaming destination type \(kafka\) | Yes |
| endpoint | Kafka broker addresses. | Yes |
| topic | Kafka topic name. | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```go
      - name: kafka-integration
        type: kafka
        endpoint: https://dc.services.visualstudio.com/v2/track
        endpoint: <kafka broker address-1>,<kafka broker address-2>
        topic: topic
```

### SignalFx

If enabled, the SignalFx integration will stream analytics and insights to an SignalFx endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Streaming destination type \(signalfx\) | Yes |
| endpoint | SignalFx endpoint. | Yes |
| token | SignalFx API token. | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric", "edac", "cluster", "topk" or "all". If you don't provide any value then it is all. | No |

```go
      - name: signalfx-integration
        type: signalfx
        endpoint: https://ingest.us1.signalfx.com/v2
        token: "<add signalfx api token>"
        features: "metric,log"
```

## Trigger Destinations

### **Slack**

If enabled, the Slack integration will stream notifications and alerts to the specified Slack channel

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Trigger destination type \(slack\) | Yes |
| endpoint | Slack Webhook or APP endpoint URL | Yes |

```go
      - name: slack-integration
        type: slack
        endpoint: "https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX"
```

**Getting started with Slack Incoming Webhooks:** [https://api.slack.com/messaging/webhooks](https://api.slack.com/messaging/webhooks)

### Examples

```go
outputs:
  streams:
      - name: sumo-logic-integration
        type: sumologic
        endpoint: "https://[SumoEndpoint]/receiver/v1/http/[UniqueHTTPCollectorCode]"
  triggers:
      - name: slack-integration
        type: slack
        endpoint: "https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX"
```

## Archive Destinations

### **AWS S3**

If enabled, the AWS S3 integration will stream logs to an AWS S3 endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Archive destination type \(s3\) | Yes |
| bucket | Target s3 bucket to send archived logs | Yes |
| region | The specified s3 bucket's region | Yes |
| aws_key_id | AWS key id that has PutObject permission to target bucket. How do I create an AWS access key? [https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key](https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key) | Yes |
| aws_sec_key | AWS secret key id that has PutObject permission to target bucket. How do I create an AWS access key? [https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key](https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key)| Yes |

```go
      - name: my-s3
        type: s3
        bucket: testbucket
        region: us-east-2
        aws_key_id: "<add aws key id>"
        aws_sec_key: "<add aws secure key>"
```

### **Azure Blob Storage**

If enabled, the Azure Blob Storage integration will stream logs to an Azure Blob Storage endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Archive destination type \(blob\) | Yes |
| account_name | Account Name for the azure account. | Yes |
| account_key | Account Key for azure account. You can visit [https://docs.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?tabs=azure-portal](https://docs.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?tabs=azure-portal) | Yes |
| container | Container to upload. | Yes |

```go
      - name: my-blob
        type: blob
        account_name: "<add account name>"
        account_key: "<add account key>"
        container: testcontainer
```

### **Google Cloud Storage**

If enabled, the Google Cloud Storage integration will stream logs to an GCS endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Archive destination type \(gcs\) | Yes |
| bucket | Target gcs bucket to send archived logs. | Yes |
| hmac_access_key | GCS HMAC Access Key which has permissions to upload files to specified bucket. See [https://cloud.google.com/storage/docs/authentication/managing-hmackeys](https://cloud.google.com/storage/docs/authentication/managing-hmackeys) for details on how to create new keys | Yes |
| hmac_secret | GCS HMAC secret associated with the access key specified. | Yes |

```go
      - name: my-gcs
        type: gcs
        bucket: ed-test-bucket
        hmac_access_key: my_hmac_access_key_123
        hmac_secret: my_hmac_secret_123
```