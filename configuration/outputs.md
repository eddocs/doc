---
description: >-
  This document outlines the various Output types (Streaming and Triggers)
  supported by the Edge Delta agent, and how the outputs are configured.
---

# Outputs

## Overview

Outputs are the mechanism that tells the Edge Delta agent which destinations to stream data, alerts, notifications, and automation. Outputs come in two forms, **Streaming** destinations, and **Trigger** destinations.

\*\*\*\*[**Streaming destinations**](outputs.md#streaming-destinations) are typically Centralized Monitoring Platforms \(i.e. Splunk, Sumo Logic, Datadog, Snowflake, New Relic, Elastic, etc.\) that Edge Delta can be configured to send the analytics and insights continuously generated by the service.

\*\*\*\*[**Trigger destinations**](outputs.md#trigger-destinations) are alerting and automation systems \(i.e. PagerDuty, Slack, ServiceNow, OpsGenie, Runbook, etc.\) that Edge Delta can be configured to send alerts and notifications when anomalies are detected or various conditions are met.

## Streaming Destinations

## Splunk

If enabled, the Splunk integration will stream analytics and insights to a Splunk HEC endpoint.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Streaming destination type \(i.e. splunk, sumologic, datadog, etc.\) | Yes |
| endpoint | Full Splunk HEC URI for this integration | Yes |
| token | Splunk HEC Token for this integration | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric" or "all". If you don't provide any value then it is all. | No |

```go
      - name: splunk-integration
        type: splunk
        endpoint: "<protocol>://<host>:<port>/<endpoint>"
        token: "32-character GUID token"
```

**Finding the appropriate HEC URI to provide for endpoint \(Splunk Enterprise, Self-Service Splunk Cloud, Managed Splunk Cloud\):** [https://docs.splunk.com/Documentation/Splunk/8.0.3/Data/UsetheHTTPEventCollector\#Send\_data\_to\_HTTP\_Event\_Collector\_on\_Splunk\_Enterprise](https://docs.splunk.com/Documentation/Splunk/8.0.3/Data/UsetheHTTPEventCollector#Send_data_to_HTTP_Event_Collector_on_Splunk_Enterprise)

**Generate HEC token for a Splunk endpoint:** [https://docs.splunk.com/Documentation/Splunk/8.0.3/Data/UsetheHTTPEventCollector\#About\_Event\_Collector\_tokens](https://docs.splunk.com/Documentation/Splunk/8.0.3/Data/UsetheHTTPEventCollector#About_Event_Collector_tokens)

**Find the HTTP Port Number used for HEC endpoints \(Global Settings\):** [https://docs.splunk.com/Documentation/Splunk/8.0.3/Data/UsetheHTTPEventCollector\#Enable\_HTTP\_Event\_Collector](https://docs.splunk.com/Documentation/Splunk/8.0.3/Data/UsetheHTTPEventCollector#Enable_HTTP_Event_Collector)

## Sumo Logic

If enabled, the Sumo Logic integration will stream analytics and insights to a Sumo Logic HTTPs Endpoint

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Streaming destination type \(i.e. sumologic, datadog, splunk, etc.\) | Yes |
| endpoint | Full HTTPs URL for this endpoint | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric" or "all". If you don't provide any value then it is all. | No |

```go
      - name: sumo-logic-integration
        type: sumologic
        endpoint: "https://[SumoEndpoint]/receiver/v1/http/[UniqueHTTPCollectorCode]"
```

**Creating a new Sumo Logic HTTPs Endpoint:** [https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source\#access-a-sources-url](https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source#access-a-sources-url) _\*\*_

**Finding an existing Sumo Logic HTTPs Endpoint URL:** [https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source\#access-a-sources-url](https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source#access-a-sources-url) _\*\*_

## Amazon CloudWatch

If enabled, the CloudWatch integration will stream logs to a given aws region.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Streaming destination type \(i.e. sumologic, datadog, splunk, etc.\) | Yes |
| region | AWS region destination for logs | Yes |
| log\_group\_name | CloudWatch log group name | Yes |
| log\_stream\_name | CloudWatch log stream name \(either name or prefix is supported not both\) | Yes |
| log\_stream\_prefix | CloudWatch log stream prefix \(either name or prefix is supported not both\) | Yes |
| auto\_create | When necessary iam policies provided if auto\_create is set, log group and log stream will be created if not exists | No |
| allow\_label\_override | monitored container can override the default values of log group name, logs stream name and log stream prefix, by setting ed\_log\_group\_name, ed\_log\_stream\_name, ed\_log\_stream\_prefix labels | No |
| auto\_configure | only supported for ECS environments, and when provided only region configuration can be provided. Automatically create LogGroupName in the format of /ecs/task\_definition\_family and LogsStreamPrefix in the format of ecs/container\_name/task\_id | No |
| type | Streaming destination type \(i.e. sumologic, datadog, splunk, etc.\) | Yes |
| features | Features defines which data types stream to backend, it can only be "log" at the moment. | No |

```go
      - name: cloudwatch
        type: cloudwatch
        region: us-west-2
        log_group_name: /ecs/microservice
        log_stream_prefix: ecs
        auto_create: true
        features: log
```

* Assign below permission to taskExecutionRoleArn for putting log events into CloudWatch when auto\_create is not set

  ```go
      {
        "Version": "2012-10-17",
        "Statement": [{
          "Effect": "Allow",
          "Action": [
            "logs:PutLogEvents"
          ],
          "Resource": "*"
        }]
      }
  ```

* Assign below permission to taskExecutionRoleArn if auto\_create is set

  ```go
      {
        "Version": "2012-10-17",
        "Statement": [{
          "Effect": "Allow",
          "Action": [
            "logs:CreateLogStream",
            "logs:CreateLogGroup",
            "logs:DescribeLogStreams",
            "logs:PutLogEvents"
          ],
          "Resource": "*"
        }]
      }
  ```

**CloudWatch log group name requirements:** [https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API\_CreateLogGroup.html](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_CreateLogGroup.html) _\*\*_

**CloudWatch log stream name requirements:** [https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API\_CreateLogStream.html](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_CreateLogStream.html) _\*\*_

## **Datadog**

If enabled, the Datadog integration will stream analytics and insights to your Datadog environment

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Streaming destination type \(i.e. datadog, sumologic, splunk, etc.\) | Yes |
| api\_key | Datadog API Key | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric" or "all". If you don't provide any value then it is all. | No |

```go
      - name: datadog-integration
        type: datadog
        api_key: "<add datadog api key>"
```

**Create a new Datadog API Key:** [https://docs.datadoghq.com/account\_management/api-app-keys/\#add-a-key](https://docs.datadoghq.com/account_management/api-app-keys/#add-a-key)

## **New Relic**

If enabled, the New Relic integration will stream analytics and insights to your New Relic environment

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Streaming destination type \(i.e. newrelic, sumologic, datadog, etc.\) | Yes |
| api\_key | New Relic Insert API Key | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric" or "all". If you don't provide any value then it is all. | No |

```go
      - name: new-relic-integration
        type: newrelic
        api_key: "<add new relic insert api key>"
```

**Create a new New Relic Insert API Key:** [https://docs.newrelic.com/docs/apis/get-started/intro-apis/types-new-relic-api-keys\#event-insert-key](https://docs.newrelic.com/docs/apis/get-started/intro-apis/types-new-relic-api-keys#event-insert-key)

## **InfluxDB**

If enabled, the InfluxDB integration will stream analytics and insights to your InfluxDB deployment

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Streaming destination type \(i.e. influxdb, sumologic, datadog, etc.\) | Yes |
| endpoint | InfluxDB endpoint | Yes |
| http\_user | InfluxDB user credentials | Yes |
| http\_password | InfluxDB password for connecting user | Yes |
| db | Specific InfluxDB database to stream data to | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric" or "all". If you don't provide any value then it is all. | No |

```go
      - name: influxdb-integration
        type: influxdb
        endpoint: "https://influxdb.<your-domain>.com/"
        port: 443
        http_user: admin
        http_password: your_http_password
        db: "specific_influxdb_database"
```

## **Wavefront**

If enabled, the Wavefront integration will stream analytics and insights to your Wavefront environment

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Streaming destination type \(i.e. wavefront, influxdb, sumologic, datadog, etc.\) | Yes |
| endpoint | Wavefront endpoint | Yes |
| token | Wavefront API token | Yes |
| features | Features defines which data types stream to backend. Only "metric" is supported. | No |

```go
      - name: wavefront
        type: wavefront
        endpoint: "https://{your wavefront domain}.wavefront.com/report"
        token: "<add wavefront api token>"
```

## **Scalyr**

If enabled, the Scalyr integration will stream analytics and insights to your Scalyr environment

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Streaming destination type \(i.e. scalyr, influxdb, sumologic, datadog, etc.\) | Yes |
| endpoint | Scalyr endpoint | Yes |
| features | Features defines which data types stream to backend, it can be "log", "metric" or "all". If you don't provide any value then it is all. | No |

```go
      - name: scalyr
        type: scalyr
        endpoint: "https://app.scalyr.com/api/uploadLogs?token={scalyr log access write key}"
```

## **Elastic Search**

If enabled, the Elastic Search integration will stream analytics and insights to your Elastic Search environment. Elastic index template and lifecycle creation guide can be found [here](../appendices/elastic-index.md). It's not mandatory but highly recommended to complete those steps in the guide to prepare your Elastic Search environment to be Edgedelta streaming target.

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Streaming destination type \(i.e. elastic, scalyr, influxdb, sumologic, datadog, etc.\) | Yes |
| index | Name of elastic index \(or index template\) where data will be streamed by edgedelta agents. Set this to 'ed-agent-log' if followed the guide above | Yes |
| cloud\_id | Cloud ID of elastic search backend | No |
| address | Address list of elastic search backend | No |
| token | Elastic Search API Key | No |
| user | Username for elastic search credentials | No |
| password | Elastic Search password for connecting user | No |
| features | Features defines which data types stream to backend, it can be "log", "metric" or "all". If you don't provide any value then it is all. | No |

* For the connection url, you can not provide cloud\_id and address at the same time. And you must provide at least one of them.
* For the authentication, you can not provide token and user/password at the same time. And you must provide at least one of them.

```go
      - name: elastic
        type: elastic
        index: "index name"
        # you can provide cloud or adress list but not both at the same time
        cloud_id: "<add elasticsearch cloud_id>"
        #address:
         #- <elastic search endpoint address_1>
         #- <elastic search endpoint address_2>
        # you can provide token or user/pass for auth but not both at the same time
        token: "elastic search api key"
        #user: "elastic search username"
        #password: "elastic search password"
```

## Trigger Destinations

## **Slack**

If enabled, the Slack integration will stream notifications and alerts to the specified Slack channel

| Key | Description | Required |
| :--- | :--- | :--- |
| name | User defined name of this specific destination, used for mapping this destination to a workflow | Yes |
| type | Trigger destination type \(i.e. slack, etc.\) | Yes |
| endpoint | Slack Webhook or APP endpoint URL | Yes |

```go
      - name: slack-integration
        type: slack
        endpoint: "https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX"
```

**Getting started with Slack Incoming Webhooks:** [https://api.slack.com/messaging/webhooks](https://api.slack.com/messaging/webhooks)

## Examples

```go
outputs:
  streams:
      - name: sumo-logic-integration
        type: sumologic
        endpoint: "https://[SumoEndpoint]/receiver/v1/http/[UniqueHTTPCollectorCode]"
  triggers:
      - name: slack-integration
        type: slack
        endpoint: "https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX"
```

